{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "PP3.3 - Proposição e Avaliação de RNAs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-IPPbpROBoi"
      },
      "source": [
        "## Redes Neurais Artificiais 2020.1\n",
        "\n",
        "**Disciplina**: Redes Neurais Artificiais 2020.1  \n",
        "**Professora**: Elloá B. Guedes (ebgcosta@uea.edu.br)  \n",
        "**Github**: http://github.com/elloa  \n",
        "        \n",
        "\n",
        "Levando em conta a base de dados **_Forest Cover Type_**, esta terceira parte do Projeto Prático 3 diz respeito à proposição e avaliação de múltiplas redes neurais artificiais do tipo feedforward multilayer perceptron para o problema da classificação multi-classe da cobertura florestal em uma área do Roosevelt National Forest.\n",
        "\n",
        "## Testando Redes Neurais sem os Atributos Categórios\n",
        "\n",
        "1. Abra a base de dados em questão\n",
        "2. Elimine todas as colunas relativas aos atributos categóricos\n",
        "3. Armazene o atributo alvo em uma variável y e os atributos preditores em uma variável X\n",
        "4. Efetue uma partição holdout 70/30 com o sklearn, distribuindo os exemplos de maneira aleatória\n",
        "5. Efetue o escalonamento dos atributos\n",
        "\n",
        "### Escalonando os atributos\n",
        "\n",
        "O treinamento de uma rede neural artificial é mais eficiente quando os valores que lhes são fornecidos como entrada são pequenos, pois isto favorece a convergência. Isto é feito escalonando-se todos os atributos para o intervalo [0,1], mas precisa ser feito de maneira cautelosa, para que informações do conjunto de teste não sejam fornecidas no treinamento.\n",
        "\n",
        "Há duas estratégias para tal escalonamento: normalização e padronização. Ambas possuem características particulares, vantagens e limitações, como é possível ver aqui: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
        "\n",
        "\n",
        "No nosso caso, vamos usar a padronização. Assim, com os atributos preditores do treinamento, isto é, X_train, deve-se subtrair a média e dividir pelo desvio padrão:\n",
        "\n",
        "X_train_std = (X_train - np.mean(X_train))/np.std(X_train)\n",
        "\n",
        "Em seguida, o mesmo deve ser feito com os atributos preditores do conjunto de testes, mas com padronização relativa ao conjunto de treinamento:\n",
        "\n",
        "X_test_std = (X_test - np.mean(X_train))/np.std(X_train)\n",
        "\n",
        "Se todo o conjunto X for utilizado na padronização, a rede neural receberá informações do conjunto de teste por meio da média e variância utilizada para preparar os dados de treinamento, o que não é desejável.\n",
        "\n",
        "\n",
        "### Continuando\n",
        "\n",
        "5. Treine uma rede neural multilayer perceptron para este problema com uma única camada e dez neurônios  \n",
        "    5.1 Utilize a função de ativação ReLU  \n",
        "    5.2 Utilize o solver Adam    \n",
        "    5.3 Imprima o passo a passo do treinamento    \n",
        "    5.4 Utilize o número máximo de épocas igual a 300  \n",
        "6. Com o modelo em questão, após o treinamento, apresente:  \n",
        "    6.1 Matriz de confusão para o conjunto de teste  \n",
        "    6.2 Acurácia  \n",
        "    6.3 F-Score  \n",
        "    6.4 Precisão  \n",
        "    6.5 Revocação  \n",
        "7. Repita o treinamento da mesma rede anterior sem imprimir o passo a passo (verbose False) por 100 vezes  \n",
        "    7.1 Cada uma destas repetições deve ser feita com uma nova partição Holdout  \n",
        "    7.2 Apresente a média e o desvio padrão da acurácia e do F-Score para o conjunto de treino  \n",
        "8. Repita por 100 vezes o treinamento desta mesma rede, mas utilizando o otimizador SGD  \n",
        "    8.1 Apresente a média e o desvio padrão da acurácia e do F-Score para o conjunto de treino  \n",
        "9. Houve influência da escolha do otimizador no desempenho da rede?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1laST-NOBok"
      },
      "source": [
        "## Reservado para a importação de bibliotecas\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objects as go\n",
        "import math\n",
        "from prettytable import PrettyTable  \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG8mF9yzOynH",
        "outputId": "c1fab8d8-349a-4dd4-d89b-381241900f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Montagem do drive para o carregamento da base de dados por meio do google colab\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWmV3nMGO0FG"
      },
      "source": [
        "# Leitura do dataset covtype.csv\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/covtype.csv', sep=',')  # caso use google colab\n",
        "# df = pd.read_csv('./covtype.csv')                                               # caso faça localmente pelo jupyter"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-gwBze5Pt13"
      },
      "source": [
        "#### Preparação dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8SieorfPuL-"
      },
      "source": [
        "# Eliminacao das colunas relativas aos atributos categoricos\n",
        "\n",
        "atributosCategoricos = []\n",
        "for i in range(40):                                                               # loop para preencher um vetor com os atributos categoricos\n",
        "  if i <=3:\n",
        "    atributosCategoricos.append(\"Wilderness_Area\"+str(i+1))\n",
        "  atributosCategoricos.append(\"Soil_Type\"+str(i+1))\n",
        "\n",
        "df = df.drop(columns=atributosCategoricos)                                        # delecao dos atributos categoricos"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiCQz7dNS6j7"
      },
      "source": [
        "y_alvo = df[\"Cover_Type\"]                                                         # separacao do atributo alvo\n",
        "x_preditor = df.drop(columns=[\"Cover_Type\"])                                      # separacao dos atributos preditores\n",
        "\n",
        "# Particao holdout para teste e treino\n",
        "x_train, x_test, y_train, y_test = train_test_split(                              # Criacao das particoes para treino e teste com o auxilio da biblioteca sklearn\n",
        "    x_preditor, y_alvo, test_size=0.3, train_size=0.7, random_state=42)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKWq_wmUXP0I"
      },
      "source": [
        "# Escalonamento usando o metodo da padronização\n",
        "\n",
        "X_train_std = (x_train - np.mean(x_train))/np.std(x_train)                        # Escalonamento do conjunto de treino \n",
        "X_test_std = (x_test - np.mean(x_train))/np.std(x_train)                          # escalonamento do conjunto de teste levando em consideracao o conjunto de treino"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_laeWQ7PnVSQ"
      },
      "source": [
        "#### Criação e treinamento da rede neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpQItHMXnVlm",
        "outputId": "55bd9713-5285-49a3-a618-84d018fecec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# criacao e treino da rede neural multilayer perceptron\n",
        "clf = MLPClassifier(hidden_layer_sizes=(10),activation=\"relu\", solver=\"adam\", \n",
        "                    random_state=1, max_iter=300, \n",
        "                    verbose=True).fit(X_train_std, y_train)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.92677146\n",
            "Iteration 2, loss = 0.70663965\n",
            "Iteration 3, loss = 0.68742275\n",
            "Iteration 4, loss = 0.67804785\n",
            "Iteration 5, loss = 0.67173330\n",
            "Iteration 6, loss = 0.66795662\n",
            "Iteration 7, loss = 0.66536055\n",
            "Iteration 8, loss = 0.66363719\n",
            "Iteration 9, loss = 0.66249517\n",
            "Iteration 10, loss = 0.66159913\n",
            "Iteration 11, loss = 0.66098261\n",
            "Iteration 12, loss = 0.66031835\n",
            "Iteration 13, loss = 0.65972541\n",
            "Iteration 14, loss = 0.65897054\n",
            "Iteration 15, loss = 0.65844078\n",
            "Iteration 16, loss = 0.65794027\n",
            "Iteration 17, loss = 0.65739333\n",
            "Iteration 18, loss = 0.65696161\n",
            "Iteration 19, loss = 0.65668155\n",
            "Iteration 20, loss = 0.65645934\n",
            "Iteration 21, loss = 0.65621898\n",
            "Iteration 22, loss = 0.65608783\n",
            "Iteration 23, loss = 0.65586838\n",
            "Iteration 24, loss = 0.65560923\n",
            "Iteration 25, loss = 0.65548498\n",
            "Iteration 26, loss = 0.65533209\n",
            "Iteration 27, loss = 0.65523662\n",
            "Iteration 28, loss = 0.65506551\n",
            "Iteration 29, loss = 0.65495128\n",
            "Iteration 30, loss = 0.65482173\n",
            "Iteration 31, loss = 0.65462318\n",
            "Iteration 32, loss = 0.65441522\n",
            "Iteration 33, loss = 0.65420575\n",
            "Iteration 34, loss = 0.65409278\n",
            "Iteration 35, loss = 0.65398200\n",
            "Iteration 36, loss = 0.65388711\n",
            "Iteration 37, loss = 0.65366403\n",
            "Iteration 38, loss = 0.65366170\n",
            "Iteration 39, loss = 0.65345990\n",
            "Iteration 40, loss = 0.65334381\n",
            "Iteration 41, loss = 0.65317999\n",
            "Iteration 42, loss = 0.65306211\n",
            "Iteration 43, loss = 0.65294690\n",
            "Iteration 44, loss = 0.65281171\n",
            "Iteration 45, loss = 0.65255375\n",
            "Iteration 46, loss = 0.65230696\n",
            "Iteration 47, loss = 0.65146953\n",
            "Iteration 48, loss = 0.65018729\n",
            "Iteration 49, loss = 0.64947215\n",
            "Iteration 50, loss = 0.64890623\n",
            "Iteration 51, loss = 0.64870961\n",
            "Iteration 52, loss = 0.64842781\n",
            "Iteration 53, loss = 0.64826938\n",
            "Iteration 54, loss = 0.64814743\n",
            "Iteration 55, loss = 0.64807387\n",
            "Iteration 56, loss = 0.64787974\n",
            "Iteration 57, loss = 0.64788750\n",
            "Iteration 58, loss = 0.64774139\n",
            "Iteration 59, loss = 0.64770944\n",
            "Iteration 60, loss = 0.64765144\n",
            "Iteration 61, loss = 0.64763910\n",
            "Iteration 62, loss = 0.64747757\n",
            "Iteration 63, loss = 0.64743926\n",
            "Iteration 64, loss = 0.64747247\n",
            "Iteration 65, loss = 0.64739861\n",
            "Iteration 66, loss = 0.64734561\n",
            "Iteration 67, loss = 0.64731831\n",
            "Iteration 68, loss = 0.64728401\n",
            "Iteration 69, loss = 0.64721641\n",
            "Iteration 70, loss = 0.64727512\n",
            "Iteration 71, loss = 0.64720340\n",
            "Iteration 72, loss = 0.64718050\n",
            "Iteration 73, loss = 0.64715420\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5N-0MrNoBE9",
        "outputId": "00da6589-a521-4615-d884-9ce19eae557d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# obtencao das respostas para o conjunto de teste\n",
        "y_pred = clf.predict(X_test_std)\n",
        "\n",
        "# print da matriz de confusao\n",
        "matrizConfusao = confusion_matrix(y_test, y_pred)                                 # calculo da matriz de confusao\n",
        "\n",
        "table = PrettyTable([\"\",\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"])                       # Cria a tabela com as colunas de 1 a 7\n",
        "table.padding_width = 1\n",
        "\n",
        "for i in range(len(matrizConfusao)):                                              # loop para preencher a tabela com os dados\n",
        "  table.add_row(np.concatenate([[str(i+1)], matrizConfusao[i]]))\n",
        "\n",
        "print(\"----------------- MATRIZ DE CONFUSAO -----------------\")                   # print da matriz de confusao\n",
        "print(table)\n",
        "\n",
        "print(\"F1-SCORE: {}\".format(round(f1_score(y_test, y_pred,                        # calculo do f-score\n",
        "                                           average='micro'), 4))) \n",
        "print(\"ACURACIA: {}\".format(round(accuracy_score(y_test, y_pred), 4)))            # calculo da acuracia\n",
        "print(\"PRECISION: {}\".format(round(precision_score(y_test, y_pred,                # calculo da precisao\n",
        "                                                   average='micro'), 4)))\n",
        "print(\"RECALL: {}\".format(round(recall_score(y_test, y_pred,                      # calculo da revocacao\n",
        "                                             average='micro'), 4)))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------- MATRIZ DE CONFUSAO -----------------\n",
            "+---+-------+-------+------+-----+-----+------+------+\n",
            "|   |   1   |   2   |  3   |  4  |  5  |  6   |  7   |\n",
            "+---+-------+-------+------+-----+-----+------+------+\n",
            "| 1 | 43913 | 18200 |  8   |  0  |  8  |  9   | 1418 |\n",
            "| 2 | 15281 | 67963 | 1156 |  0  | 100 | 475  | 103  |\n",
            "| 3 |   0   |  1858 | 7512 | 131 |  0  | 1137 |  0   |\n",
            "| 4 |   0   |   4   | 375  | 271 |  0  | 145  |  0   |\n",
            "| 5 |   15  |  2547 |  46  |  0  | 323 |  10  |  0   |\n",
            "| 6 |   0   |  1463 | 2299 |  54 |  0  | 1411 |  0   |\n",
            "| 7 |  2843 |   28  |  0   |  0  |  0  |  0   | 3198 |\n",
            "+---+-------+-------+------+-----+-----+------+------+\n",
            "F1-SCORE: 0.7148\n",
            "ACURACIA: 0.7148\n",
            "PRECISION: 0.7148\n",
            "RECALL: 0.7148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aLK7ye3270j"
      },
      "source": [
        "f1scores = np.zeros(100)\n",
        "acuracias = np.zeros(100)\n",
        "\n",
        "for i in range (100):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(                              # Criacao das particoes para treino e teste com o auxilio da biblioteca sklearn\n",
        "    x_preditor, y_alvo, test_size=0.3, train_size=0.7, random_state=42)\n",
        "  \n",
        "  X_train_std = (x_train - np.mean(x_train))/np.std(x_train)                        # Escalonamento do conjunto de treino \n",
        "  X_test_std = (x_test - np.mean(x_train))/np.std(x_train)  \n",
        "\n",
        "  clf = MLPClassifier(hidden_layer_sizes=(10),activation=\"relu\", solver=\"sgd\", \n",
        "                      random_state=1, max_iter=300, \n",
        "                      verbose=False).fit(X_train_std, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test_std)\n",
        "\n",
        "  f1scores[i] = round(f1_score(y_test, y_pred, average='micro'), 4)\n",
        "  acuracias[i] = round(accuracy_score(y_test, y_pred), 4)\n",
        "\n",
        "print(\"media acuracia: {}\".format(acuracias.mean()))\n",
        "print(\"desvio padrao acuracia: {}\".format(acuracias.std()))\n",
        "print(\"media f-score: {}\".format(f1scores.mean()))\n",
        "print(\"desvio padrao f-score: {}\".format(f1scores.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Y3YAel6syn"
      },
      "source": [
        "f1scores = np.zeros(100)\n",
        "acuracias = np.zeros(100)\n",
        "\n",
        "for i in range (100):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(                              # Criacao das particoes para treino e teste com o auxilio da biblioteca sklearn\n",
        "    x_preditor, y_alvo, test_size=0.3, train_size=0.7, random_state=42)\n",
        "  \n",
        "  X_train_std = (x_train - np.mean(x_train))/np.std(x_train)                        # Escalonamento do conjunto de treino \n",
        "  X_test_std = (x_test - np.mean(x_train))/np.std(x_train)  \n",
        "\n",
        "  clf = MLPClassifier(hidden_layer_sizes=(10),activation=\"relu\", solver=\"adam\", \n",
        "                      random_state=1, max_iter=300, \n",
        "                      verbose=False).fit(X_train_std, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test_std)\n",
        "\n",
        "  f1scores[i] = round(f1_score(y_test, y_pred, average='micro'), 4)\n",
        "  acuracias[i] = round(accuracy_score(y_test, y_pred), 4)\n",
        "\n",
        "print(\"media acuracia: {}\".format(acuracias.mean()))\n",
        "print(\"desvio padrao acuracia: {}\".format(acuracias.std()))\n",
        "print(\"media f-score: {}\".format(f1scores.mean()))\n",
        "print(\"desvio padrao f-score: {}\".format(f1scores.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4P1gJcaOBon"
      },
      "source": [
        "## Discussão\n",
        "\n",
        "Nos passos anteriores, você avaliou o desempenho de uma única rede neural que contém os seguintes parâmetros: uma única camada oculta com 10 neurônios e função de ativação ReLU. O otimizador utilizado, quer seja SGD ou ADAM, trata-se do algoritmo para aproximar o gradiente do erro. Neste sentido, a escolha do otimizador é um hiperparâmetro, pois diz respeito a como a rede neural definida previamente atuará \"em tempo de execução\"  durante o processo de treinamento. Também são hiperparâmetros a quantidade de épocas, a taxa de aprendizado inicial, dentre outros.\n",
        "\n",
        "Cabe alientar também que você efetuou o treinamento desta rede por 100 vezes e apresentou os resultados em termos de média +- desvio padrão. Lembre-se que em uma rede neural há a inicialização aleatória de pesos e, em consequência, o desempenho delas está sujeito à uma flutuação estocástica. A execução destas múltiplas vezes faz com que eliminemos algum viés introduzido por uma boa ou má \"sorte\" na escolha de pesos no caso de uma única execução.\n",
        "\n",
        "## Propondo Novas Arquiteturas\n",
        "\n",
        "Variando  os parâmetros (uma ou duas camadas ocultas, com diferente números de neurônios em cada uma delas e a função de ativação) e o hiperparâmetros solver (Adam ou SGD) e o número de épocas (100,150 e 200), atenda ao que se pede:\n",
        "\n",
        "1. Proponha 10 arquiteturas distintas de RNAs para o problema em questão, à sua escolha\n",
        "2. Avalie cada uma das arquiteturas perante todos os hiperparâmetros apresentados por 100 vezes\n",
        "3. Como resultado da avaliação, apresente:  \n",
        "    3.1 Top-3 melhores redes no tocante à F-Score e Acurácia  \n",
        "    3.2 Repetição em que houve o melhor desempenho de cada uma dessas redes: ilustre tp, tf, fp e fn  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIj2ptKOBon"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olxa7kX2OBoq"
      },
      "source": [
        "## Estimando o número de neurônios\n",
        "\n",
        "Um dos problemas de pesquisa com redes neurais artificiais consiste na determinação do número de neurônios em sua arquitetura. Embora não seja possível definir a priori qual rede neural é adequada para um problema, pois isto só é possível mediante uma busca exaustiva, há regras na literatura que sugerem o número de neurônios escondidos, tal como a regra da Pirâmide Geométrica, dada a seguir:\n",
        "\n",
        "$$N_h = \\alpha \\cdot \\sqrt{N_i \\cdot N_o},$$\n",
        "\n",
        "em que $N_h$ é o número de neurônios ocultos (a serem distribuídos em uma ou duas camadas ocultas), $N_i$ é o número de neurônios na camada de entrada e $N_o$ é o número de neurônios na camada de saída. \n",
        "\n",
        "1. Consulte a documentação da classe MLPClassifier (disponível em https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) e obtenha os valores de $N_i$ e $N_h$.\n",
        "2. Teste os valores de $\\alpha$ como sendo iguais a $0.5$, $2$ e $3$.\n",
        "3. Proponha pelo menos 30 redes neurais segundo a regra da pirâmide geométrica e teste-as nos mesmos termos estabelecidos anterioremente  (solver, épocas, etc.)  \n",
        "    3.1 Apresente as top-3 melhores redes no tocante à F-Score e Acurácia  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWkbbHy2OBoq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UehBG1nOBos"
      },
      "source": [
        "## Testando as Redes Neurais com Atributos Categóricos\n",
        "\n",
        "1. Considere as 6 redes neurais obtidas nos dois top-3 anteriores (arquiteturas próprias e regra da pirâmide geométrica)\n",
        "2. Com todos os atributos preditores da base de dados original, incluindo os categóricos, treine e teste estas mesmas redes por 100 repetições  \n",
        "    2.1 Considere o melhor otimizador para cada uma delas  \n",
        "    2.2 Faça uso de 200 épocas para treinamento  \n",
        "    2.2 Apresente os resultados de acurácia e F-Score em termos da média +- dp para cada arquitetura\n",
        "3. Apresente o gráfico boxplot para o F-Score das 6 arquiteturas perante as 100 repetições"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEvL8grsOBot"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B7-fC8IOBov"
      },
      "source": [
        "## Considerações Parciais\n",
        "\n",
        "1. É possível identificar uma rede com desempenho superior às demais?\n",
        "2. Qual estratégia mostrou-se mais producente para a obtenção de boas arquiteturas (Estratégia Própria ou Pirâmide Geométrica)? Por quê?\n",
        "3. Considerar os atributos categóricos trouxe melhorias? Justifique.\n",
        "4. Um número maior de épocas trouxe melhorias?\n",
        "5. Qual a maior dificuldade de resolução do problema proposto perante as RNAs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItP1HTw8OBow"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}